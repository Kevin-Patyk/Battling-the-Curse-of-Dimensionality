---
title: "Practical 5"
author: "Kevin Patyk"
date: "12/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```

# Introduction

In this practical, we will apply hierarchical and k-means clustering to two synthetic datasets. We use the following packages:
```{r message=FALSE, warning=FALSE}
library(MASS)
library(tidyverse)
library(patchwork)
library(ggdendro)
```

The data can be generated by running the code below.

**1: The code does not have comments. Add descriptive comments to the code below.**
```{r}
#setting a seed for reproducibility 
set.seed(123) 

#this is making a 2x2 correlation matrix for the variables
sigma      <- matrix(c(1, .5, .5, 1), 2, 2)
#this is simulating multivariate normal data with 100 observations, means 5, and a covariance matrix equal to the 2x2 matrix we specified in the previous line 
sim_matrix <- mvrnorm(n = 100, mu = c(5, 5), 
                      Sigma = sigma)
#renaming the column names to x1 and x2
colnames(sim_matrix) <- c("x1", "x2") 

#making a data frame/tibble out of the matrix we simulated using mvrnorm and also adding a class column, with options A, B, or C
sim_df <- 
  sim_matrix %>% 
  as_tibble() %>%
  mutate(class = sample(c("A", "B", "C"), size = 100, 
                        replace = TRUE)) 

#now we are making a new data frame and changing x1 and x2 a little bit; so, for example, when it is case A in x2 we will add .5; if it is case A in x1 we will subtract .5 
sim_df_small <- sim_df %>%
  mutate(x2 = case_when(class == "A" ~ x2 + .5,
                        class == "B" ~ x2 - .5,
                        class == "C" ~ x2 + .5),
         x1 = case_when(class == "A" ~ x1 - .5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + .5))

#now we are making a new data frame and changing x1 and x2 a little bit; so, for example, when it is case A in x2 we will add 2.5; if it is case A in x1 we will subtract 2.5 - the values have changed for what the true underlying class would be 
sim_df_large <- 
  sim_df %>%
  mutate(x2 = case_when(class == "A" ~ x2 + 2.5,
                        class == "B" ~ x2 - 2.5,
                        class == "C" ~ x2 + 2.5),
         x1 = case_when(class == "A" ~ x1 - 2.5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + 2.5))
```

**2: Prepare two unsupervised datasets by removing the class feature.**
```{r}
#remvoing class for small
sim_df_small <- sim_df_small %>%
  select(-class)

#removing class for large 
sim_df_large <- sim_df_large %>%
  select(-class)
```

**3: For each of these datasets, create a scatterplot. Combine the two plots into a single frame (look up the `patchwork` package to see how to do this!) What is the difference between the two datasets?**
```{r}
#plot for small
p1 <- sim_df_small %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point() +
  ggtitle("sim_df_small") +
  coord_fixed() + #fixes distances on the x and y axes
  xlim(0, 10) +
  ylim(0,10)

#plot for large
p2 <- sim_df_large %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point() +
  ggtitle("sim_df_large") +
  coord_fixed() + #fixes distances on the x and y axes
  xlim(0, 10) +
  ylim(0,10)

#displaying the plots
p1 + p2
```

The `sim_df_small` dataset has a lot of class overlap and no distinct clusters, while the `sim_df_large` dataset has less class overlap and more distinct clusters.

-----

#  Hierarchical clustering

**4: Run a hierarchical clustering on these datasets and display the result as dendrograms. Use euclidian distances (first hyperparameter) and the complete agglomeration method (second hyperparameter). Make sure the two plots have the same y-scale. What is the difference between the dendrograms? (Hint: functions youâ€™ll need are `hclust`, `ggdendrogram`, and `ylim`)**
```{r message=FALSE, warning=FALSE}
#first make dissimilarity matrix (based on euclidean distance)
dist_small <- dist(sim_df_small, method = "euclidian")
#then call hclust function on dissimilarity matrix (with complete linkage)
hc_small <- hclust(dist_small, method = "complete") 

#first make dissimilarity matrix (based on euclidean distance)
dist_large <- dist(sim_df_large, method = "euclidian")
#then call hclust function on dissimilarity matrix (with complete linkage)
hc_large <- hclust(dist_large, method = "complete")

#plotting the results
p1d <- ggdendrogram(hc_small) + 
  ylim(0, 10) +
  ggtitle("hc_small")
p2d <- ggdendrogram(hc_large) + 
  ylim(0, 10) +
  ggtitle("hc_large")

#displaying the plots
p1d + p2d
```

The dendrogram for `hc_large` is much taller than the `hc_small` dendrogram. The long branches indicate that the data points get clustered together much later in `hc_large`. This makes sense since the data used in `hc_large` has much more separation than the data used in `hc_small`, as seen in the previous question. The intercluster dissimilarity is higher for `hc_large`. From 3 to 2 clusters in `hc_large` you need to overcome a lot of dissimilarities. The longer the branches (lines) are, the more dissimilarity there is to overcome to merge the clusters. 

**5: For the dataset with small differences, also run a complete agglomeration hierarchical cluster with manhattan distance.**
```{r message=FALSE, warning=FALSE}
#first make dissimilarity matrix (based on euclidean distance)
dist_small_2 <- dist(sim_df_small, method = "manhattan")
#then call hclust function on dissimilarity matrix (with complete linkage)
hc_small_2 <- hclust(dist_small_2, method = "complete") 

#plotting the results for manhattan distance
p3d <- ggdendrogram(hc_small_2) + 
  ylim(0, 10) +
  ggtitle("hc_small_man")

#displaying the results
p1d + p3d
```

**6: Use the `cutree()` function to obtain the cluster assignments for three clusters and compare the cluster assignments to the 3-cluster euclidean solution. Do this comparison by creating two scatter plots with cluster assignment mapped to the `color` aesthetic. Which differences do you see?**
```{r}
# obtain clusters - you can select how many groups or at which height can be cut
clust_small_e <- cutree(hc_small, k = 3)
clust_small_m <- cutree(hc_small_2, k = 3)

#plotting euclidean distance model with cluster assignment
p1ce <- cbind(sim_df_small, clust_small_e) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(clust_small_e))) +
  ggtitle("euclidean") + 
  theme(legend.position = "none") 

#plotting manhattan distance model with cluster assignment
p2cm <- cbind(sim_df_small, clust_small_m) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(clust_small_m))) +
  ggtitle("manhattan") + 
  theme(legend.position = "none")

#displaying the plots
p1ce + p2cm
```

The clusterings are different. When using Manhattan distance, there seems to be a more even dispersion of data points per cluster, whereas, when using Euclidean distance, red seems to dominate the chart. Additionally, when using Euclidean distance the center of the plot (around 5, 5.5) seems to be dominated by the red points, but in the plot using Manhattan distance the center of the plot is more evenly distributed between colors. Euclidean distance clusters are generally more circular, while Manhattan clusters are more diamond. This is similar to the L1 (Manhattan) and L2 (Euclidean) norms in LASSO and ridge regression respectively.

The professor recommends using Euclidean by default and Manhattan in only very specific situations. Manhattan should be used if you have many variables and on one of the variables there is a huge outlier. Manhattan can be more robust to outliers. 

-----

# K-means clustering

**7: Create k-means clusterings with 2, 3, 4, and 6 classes on the large difference data. Again, create coloured scatter plots for these clusterings.**
```{r}
#creating the kmeans algorithm with differing numbers of clusters 
k2 <- kmeans(x = sim_df_large, centers = 2)
k3 <- kmeans(x = sim_df_large, centers = 3)
k4 <- kmeans(x = sim_df_large, centers = 4)
k6 <- kmeans(x = sim_df_large, centers = 6)

#plotting the outputs with differing number of clusters
pk2 <- cbind(sim_df_large, k2$cluster) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(k2$cluster))) +
  ggtitle("k2") + 
  theme(legend.position = "none")

pk3 <- cbind(sim_df_large, k3$cluster) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(k3$cluster))) +
  ggtitle("k3") + 
  theme(legend.position = "none")

pk4 <- cbind(sim_df_large, k4$cluster) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(k4$cluster))) +
  ggtitle("k4") + 
  theme(legend.position = "none")

pk6 <- cbind(sim_df_large, k6$cluster) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(k6$cluster))) +
  ggtitle("k6") + 
  theme(legend.position = "none")

#displaying the plots
pk2 + pk3 + pk4 + pk6
```

**8: Do the same thing again a few times. Do you see the same results every time? Where do you see differences?**
```{r}
#making storage lists for the outputs 
k22 <- list()
k32 <- list()
k42 <- list()
k62 <- list()
#running the for loop 
for(i in 1:5){
  k22[[i]] <- kmeans(x = sim_df_large, centers = 2)$cluster
  k32[[i]] <- kmeans(x = sim_df_large, centers = 3)$cluster
  k42[[i]] <- kmeans(x = sim_df_large, centers = 4)$cluster
  k62[[i]] <- kmeans(x = sim_df_large, centers = 6)$cluster
}
```

Now, to visually inspect the differences.

## Results {.tabset}

### Plot `k2`
```{r}
for(i in 1:5){
  print(cbind(sim_df_large, k22[[i]]) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(k22[[i]]))) +
  ggtitle("k2") + 
  theme(legend.position = "none"))
}
```

### Plot `k3`
```{r}
for(i in 1:5){
  print(cbind(sim_df_large, k32[[i]]) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(k32[[i]]))) +
  ggtitle("k3") + 
  theme(legend.position = "none"))
}
```

### Plot `k4`
```{r}
for(i in 1:5){
  print(cbind(sim_df_large, k42[[i]]) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(k42[[i]]))) +
  ggtitle("k4") + 
  theme(legend.position = "none"))
}
```

### Plot `k6`
```{r}
for(i in 1:5){
  print(cbind(sim_df_large, k62[[i]]) %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = as.factor(k62[[i]]))) +
  ggtitle("k6") + 
  theme(legend.position = "none"))
}
```

## {-}

**9: Find a way online to perform bootstrap stability assessment for the 3 and 6-cluster solutions.**

Creating the clustering bootstrap.
```{r message=FALSE, warning=FALSE, results='hide'}
#loading the package
library(fpc)

#bootstrapping for 3 clusters
boot_3 <- clusterboot(data = sim_df_large, B = 100, bootmethod = "boot", seed = 123, clustermethod = kmeansCBI, k = 3)
#bootstrapping for 6 clusters
boot_6 <- clusterboot(data = sim_df_large, B = 100, bootmethod = "boot", seed = 123, clustermethod = kmeansCBI, k = 6)
```

Examining the results of the cluster bootstrap. 
```{r}
#printing the results for 3 clusters 
boot_3$bootmean
#printing the results 6 clusters 
boot_6$bootmean
```

For the 3 cluster bootstrap, there is very high similarity and stability between the clusters. For the 6 cluster bootstrap, there is not very high similarity and stability between the clusters. 

-----

# Challenge question

**10: Create a function to perform k-medians clustering.**

Write this function from scratch: you may use base-`R` and `tidyverse` functions. Use Euclidean distance as your distance metric.

* Input: 
  * Dataset (as a data frame) 
  * K (number of clusters)

Output: A
  * Vector of cluster assignments

Tip: Use the unsupervised version of `sim_df_large` with `K = 3` as a tryout-dataset.

This is my code and partially the professors. 
```{r}
#converting sim_df_large to a data frame cause it is easier to work with
sim_df_large <- as.data.frame(sim_df_large)

#setting the seed to reproduce results and check results
set.seed(123)

#writing the function
kmedians <- function(df, k, max_iter = 100){
  
  #creating a vector which is a random combination of 1:k with replacement
  cluster <- sample(x = c(1:k), size = nrow(df), replace = T)
  
  #start the iteration process
  converged <- FALSE
  iter <- 0
  #now, creating a while look to keep the process going until the stopping criterion is reached
  while (converged == FALSE && iter < max_iter){
    old_clus <- cluster
    iter <- iter + 1
  
  #splitting the data frame based on the number of k and getting the median per feature using       tidyverse after adding the random cluster vector
  centroids <- df %>% 
      mutate(cluster = cluster) %>% 
      group_by(cluster) %>% 
      summarize(across(everything(), median))  %>%
      select(-cluster)
  
  #making a matrix for the distances output 
  d <- matrix(0, nrow = nrow(df), ncol = k)
  #calculate distance of each data point to each centroid and fill the empty matrix
  for (j in 1:k) { 
    for (i in 1:nrow(df)) { 
      d[i,j] <- sqrt(sum((df[i,1:ncol(df)] - centroids[j,1:ncol(centroids)])^2))
    }
  }
  
  #creating 0 vector for containing cluster number
  cluster <- numeric()
  #assigning each point to a cluster
  for(i in 1:nrow(d)){
    cluster[i] <- which.min(d[i, ])
  }
  
  #check for convergence
    if (all(old_clus == cluster)){
      converged <- TRUE
  }
  }
  
  #print some information
  cat("K-medians with K =", k, "| Iterations:", iter, "| converged:", converged, "\n")
  #return the cluster assignments
  return(cluster)
}

mine <- kmedians(df = sim_df_large, k = 3)
```

So, what this function is doing is first assigning each observation randomly to clusters 1 - 3. Then, we create a `while` loop that will keep running until either the maximum number of iterations have been reached or the new cluster is the same as the previous cluster. We calculate the centroids of the clusters using `tidyverse` functions. There are 2 `for` loops, one which calculates the distances of each point from the centroid and one which assigns each observation a cluster. This process occurs until `convergence = TRUE`. 

This is the professors-only code to use for comparison.
```{r}
#creating the distance function
l2_dist <- function(x, y) c(crossprod(x - y))

#setting the seed to reproduce results and check results
set.seed(123)

#making the function 
kmedians2 <- function(data, K, smart_init = FALSE, max_iter = 1000) {
  #getting the number of observations
  N <- nrow(data)
  
  #assign each data point to a random cluster 
  clus <- sample(K, N, replace = TRUE)
  
  #start the iteration process
  converged <- FALSE
  iter <- 0
  #now, creating a while look to keep the process going until the stopping criterion is reached
  while (converged == FALSE && iter < max_iter) {
    old_clus <- clus
    iter     <- iter + 1
    
    #compute medians of clusters using the tidyverse 
    medians <-
      data %>% 
      mutate(k = clus) %>% 
      group_by(k) %>% 
      summarize(across(everything(), median))
    
    #assign observations to closest cluster
    for (n in 1:N) {
      #get nth data point
      data_n <- unlist(data[n,])
      
      #compute distance to each cluster
      dist_n <- numeric(K)
      for (k in 1:K) {
        median_k  <- unlist(medians[k, 2:3])
        dist_n[k] <- l2_dist(data_n, median_k)
      }
      
      #assign obs to cluster with smallest distance
      clus[n] <- which.min(dist_n)
    }
    
    # check for convergence
    if (all(old_clus == clus))
      converged <- TRUE
  }
  
  # print some information
  cat("K-medians with K =", K, "| Iterations:", iter, "| converged:", converged, "\n")
  
  # return the cluster assignments
  return(clus)
}

prof <- kmedians2(sim_df_large, 3)
```

Comparing the outputs to see if I got it right.
```{r}
table(mine)
table(prof)
```

Our code matches the output given by the professors. 

The code below was me just messing around to see if I could replicate the `group_by()` function without use the `tidyverse`. 
```{r}
#first, making a data frame for testing with random cluster assignments
df <- sim_df_large
df <- df %>%
  mutate(cluster = sample(x = c(1:3), size = nrow(df), replace = T))

#setting the seed to reproduce results and check results
set.seed(123)

#then, making a double for loop to replace the group_by() function 
df2 <- list()
med_out <- numeric()
mat_out <- matrix(nrow = 3, ncol = 3)
for(i in 1:3){
df2[[i]] <- df[df$cluster == i, ]
  for(j in 1:ncol(df2[[i]])){
  med_out[j] <- round(median(df2[[i]][, j]), digits = 2)
    }
mat_out[i, ] <- med_out
}
```

Now, to compare the output to the output given by the `group_by()` function.
```{r}
#setting the seed to reproduce results and check results
set.seed(123)

#getting the centroids using the tidyverse
centroids <- df %>% 
      mutate(cluster = cluster) %>% 
      group_by(cluster) %>% 
      summarize(across(everything(), median))  %>%
      select(-cluster)

#comparing the results
mat_out
centroids
```

The output matches, so our double `for` loop worked and gives us the same results as the `group_by()` function. 

**11: Add an input parameter `smart_init`. If this is set to `TRUE`, initialize cluster assignments using hierarchical clustering (from `hclust`). Using the unsupervised `sim_df_small`, look at the number of iterations needed when you use this method vs when you randomly initialize.**
```{r}
#the only difference with this algorithm is that, if smart_init = T, the initial cluster assignments come from hierarchical clustering, not just random assignment 
kmedians3 <- function(data, K, smart_init = FALSE, max_iter = 1000) {
  
  #getting the number of observations
  N <- nrow(data)
  
  # new part
  if (!smart_init) {
    #random cluster initialization
    clus <- sample(K, N, replace = TRUE)
  } else {
    #use hclust for initialization
    clus <- cutree(hclust(dist(df_l)), k = K)
  }
  
  #this part is exactly the same as the previous code, so no new additional comments will be added 
  converged <- FALSE
  iter <- 0
  while (converged == FALSE && iter < max_iter) {
    old_clus <- clus
    iter     <- iter + 1
    medians <-
      data %>% 
      mutate(k = clus) %>% 
      group_by(k) %>% 
      summarize(across(everything(), median))
    for (n in 1:N) {
      data_n <- unlist(data[n,])
      dist_n <- numeric(K)
      for (k in 1:K) {
        median_k  <- unlist(medians[k, 2:3])
        dist_n[k] <- l2_dist(data_n, median_k)
      }
      clus[n] <- which.min(dist_n)
    }
    if (all(old_clus == clus))
      converged <- TRUE
  }
  cat("K-medians with K =", K, "| Iterations:", iter, "| converged:", converged, "\n")
  return(clus)
}

set.seed(123)
kmedians3(sim_df_small, 3, smart_init = FALSE)
```

With `smart_init = TRUE`, the algorithm takes 6 iterations to converge. The previous algorithms with randomly assigned clusters only take 3 iterations to converge.

-----

# End of document

-----

```{r}
sessionInfo()
```

